# Training Methodology

How to assess, improve, and validate skills through structured iteration.

## The Training Cycle

```
Assess â†’ Hypothesize â†’ Apply â†’ Eval â†’ Record â†’ (repeat or stop)
```

Each cycle targets **one issue**. Never stack unvalidated fixes.

## Assessment

### Reading a skill

1. **SKILL.md** â€” Is the frontmatter correct? Is the description within 1024 chars? Are stop signals present and specific?
2. **references/** â€” Are they loaded on demand or bloating the main file? Token budget: 4K orchestrating, 15K knowledge, 20K total.
3. **training-logs/<skill-name>.md** â€” What was tried before? What worked? What regressed? Lives at `plugins/<group>/training-logs/`, NOT inside the skill directory.
4. **Existing evals** â€” Check Arena for recent results. Check `evals/<skill-name>/` in the repo.

### Quick multi-model evaluation

Select 3 models from 2+ families. Current recommendations:
- **Claude Sonnet 4** or **Claude Sonnet 4.5** (Anthropic)
- **GPT-5.1-Codex** or **GPT-5.2** (OpenAI)
- **Claude Opus 4.6** (premium, for critical skills)

> âš ï¸ Gemini 3 Pro has been unreliable (400 errors). Use `gpt-5.3-codex` as the third family if needed.

Use the `task` tool with `model` parameter. Give each model the same realistic prompt. Compare:
- Correctness of output
- Tool call count and sequence
- Whether guidance was followed or misapplied

### Issue ranking

Rank findings by impact:
- **âŒ Wrong** â€” Guidance causes incorrect behavior. Fix immediately.
- **âš ï¸ Incomplete** â€” Missing guidance causes models to guess. Fix soon.
- **ğŸ’¡ Opportunity** â€” Could be better but isn't broken. Fix if time permits.

## Hypothesis-Driven Fixes

Before editing, state:
```
Hypothesis: Changing [specific text] will fix [observed problem] because [reasoning].
Evidence: [what you observed that led to this hypothesis]
Risk: [what could regress]
```

This goes in the training log whether the fix works or not.

## Validation

### Regression heuristics (from Arena)

| Metric | Flag | Signal |
|--------|------|--------|
| Tool call increase > 20% on any task | ğŸ”´ Regression | Rollback the change |
| Tool call decrease > 10% | ğŸŸ¢ Improvement | Record as evidence |
| Model misapplies new guidance | ğŸ”´ Regression | Needs anti-pattern or rewording |
| Models converge on correct behavior | ğŸŸ¢ Improvement | High confidence |
| One model improves, others unchanged | ğŸŸ¡ Partial | Likely acceptable |

### Before/after comparison

Run the same prompt on the same models. Capture:
- Output correctness (qualitative)
- Tool call count (quantitative)
- Any guidance the model cited or ignored
- Time to completion

## When to Stop

- **After 3 change-eval cycles per session.** More than that risks fatigue and compounding changes.
- **When scores are â‰¥ 4/5 across all test models.** Diminishing returns beyond this.
- **When the remaining issues are ğŸ’¡ not âŒ or âš ï¸.** Ship what you have.
- **When you're unsure if a change helps.** Open an Arena issue instead of guessing.

## Common Training Patterns

### Pattern: Stop signal is missing or vague
The highest-leverage single edit. Arena data: one stop signal sentence saved 10+ tool calls (42â†’25).
- **Bad:** "Try to be efficient"
- **Good:** "Stop when you have identified root cause. Check at most 3 work items before reporting."
- Place stop signals at decision points, not just at the top.

### Pattern: Model uses wrong tool or skips available tool
Usually means the INVOKES bridge is missing or too specific.
- Use tool **family** names, not individual tools: "INVOKES: maestro and GitHub MCP tools"
- Always list scripts â€” they're undiscoverable from the tool list
- A/B test evidence: GPT-5.1 went from 4â†’6 correct tools with family-level INVOKES

### Pattern: Model over-investigates
Add explicit numeric bounds: "Check at most N items", "Stop after finding the first match"
- Domain examples teach reasoning better than procedural steps

### Pattern: Guidance causes model to do the wrong thing
Add an inline anti-pattern near the step where the mistake occurs.
- Format: `> âŒ **NEVER** [the mistake]. [Why it's wrong]. [What to do instead].`
- Then have the *same model* explain why it made the mistake â€” its self-analysis reveals guidance gaps

### Pattern: Different models interpret guidance differently
The guidance is ambiguous. Rewrite with concrete examples.
- If one model gets it right, study what it focused on â€” that's the signal to amplify

### Pattern: Guidance references properties not in the output
If guidance says "for GitHub-hosted repos, do X" but the model can't tell which repos are GitHub-hosted from the tool output, it will guess wrong. Reference **observable signals** in the output instead (e.g., a `~` prefix, a field being null, a specific format).
- âœ… "Entries with `~` prefix need manual computation" (model can see the `~`)
- âŒ "GitHub-hosted repos show real distances" (model can't determine hosting from the output)
- Quantify consequences: "overstates by 10x-300x" anchors models better than "meaningless number"
